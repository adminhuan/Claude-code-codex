"""
åŸºç¡€ä»£ç†æŠ½è±¡ç±»
å®šä¹‰æ‰€æœ‰ä»£ç†çš„æ ‡å‡†æ¥å£
æŒ‰ç…§Codexå»ºè®®å®ç°ä¸¥æ ¼çš„JSONåè®®çº¦æŸ
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import json
import re
import asyncio
from ..protocols.conversation import (
    OrchestratorPrompt,
    AgentReply,
    validate_agent_reply,
    create_error_reply,
    AGENT_REPLY_SCHEMA,
    Phase
)


class BaseAgent(ABC):
    """æ‰€æœ‰ä»£ç†çš„åŸºç¡€æŠ½è±¡ç±» - å¼ºåˆ¶JSONåè®®"""

    def __init__(self, model_name: str = ""):
        self.model_name = model_name
        self.token_count = 0
        self.retry_count = 0
        self.max_retries = 3

    @abstractmethod
    async def _raw_generate(self, prompt: OrchestratorPrompt) -> str:
        """
        åŸå§‹ç”Ÿæˆæ–¹æ³• - å­ç±»å®ç°
        è¿”å›æ¨¡å‹çš„åŸå§‹æ–‡æœ¬è¾“å‡º
        """
        pass

    async def generate(self, prompt: OrchestratorPrompt) -> AgentReply:
        """
        ç”Ÿæˆå›å¤çš„æ ¸å¿ƒæ–¹æ³• - å¸¦é”™è¯¯å¤„ç†å’Œé‡è¯•
        Args:
            prompt: æ ‡å‡†åŒ–çš„åè°ƒå™¨æç¤º
        Returns:
            AgentReply: ä¸¥æ ¼éªŒè¯çš„ç»“æ„åŒ–å›å¤
        """
        for attempt in range(self.max_retries):
            try:
                # 1. è·å–åŸå§‹è¾“å‡º
                raw_response = await self._raw_generate(prompt)

                # 2. è§£æä¸ºJSON
                parsed_json = self._extract_json(raw_response)

                # 3. ä¸¥æ ¼éªŒè¯
                validated_reply = validate_agent_reply(parsed_json)

                return validated_reply

            except Exception as e:
                self.retry_count += 1

                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å›é”™è¯¯å›å¤
                if attempt == self.max_retries - 1:
                    return create_error_reply(
                        f"ä»£ç†ç”Ÿæˆå¤±è´¥ï¼ˆå°è¯•{self.max_retries}æ¬¡ï¼‰: {str(e)}",
                        prompt.current_phase
                    )

                # å‡†å¤‡é‡è¯• - é™ä½æ¸©åº¦å’Œç®€åŒ–æç¤º
                await asyncio.sleep(0.5 * (attempt + 1))  # æŒ‡æ•°é€€é¿

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œ
        return create_error_reply("æ„å¤–é”™è¯¯", prompt.current_phase)

    def _extract_json(self, response_text: str) -> Dict[str, Any]:
        """
        ä»å“åº”ä¸­æå–JSON - æ”¯æŒå¤šç§æ ¼å¼
        """
        cleaned = response_text.strip()

        # 1. å°è¯•æå–JSONä»£ç å—
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', cleaned, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # 2. å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } çš„å†…å®¹
            start = cleaned.find('{')
            end = cleaned.rfind('}')
            if start != -1 and end != -1 and end > start:
                json_str = cleaned[start:end+1]
            else:
                # 3. æ•´ä¸ªå“åº”å¯èƒ½å°±æ˜¯JSON
                json_str = cleaned

        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            raise ValueError(f"JSONè§£æå¤±è´¥: {str(e)}ã€‚åŸå§‹å“åº”: {response_text[:200]}...")

    def _build_system_prompt(self, role_prompt: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º - å¼ºåˆ¶JSONè¾“å‡º"""
        base_system = f"""ä½ æ˜¯ä¸€ä¸ªåä½œAIä»£ç†ã€‚å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼å›å¤ï¼Œå¦åˆ™è¢«é©³å›é‡è¯•ã€‚

JSON Schema (ä¸¥æ ¼éµå¾ª):
{json.dumps(AGENT_REPLY_SCHEMA, indent=2, ensure_ascii=False)}

å…³é”®è¦æ±‚:
1. å›å¤å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬
2. å¿…é¡»åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼šphase, message, finish
3. messageå­—æ®µé™åˆ¶åœ¨500å­—ç¬¦ä»¥å†…
4. åªä½¿ç”¨æŒ‡å®šçš„æšä¸¾å€¼
5. ä¸è¦è§£é‡Šæˆ–æ·»åŠ æ³¨é‡Šï¼Œåªè¿”å›JSON

ç¤ºä¾‹æœ‰æ•ˆå›å¤:
{{
    "phase": "analysis",
    "message": "å·²åˆ†æé—®é¢˜ï¼Œå‘ç°éœ€è¦ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦",
    "tool_calls": [],
    "finish": "handoff",
    "critiques": "è¯·å®¡æŸ¥è¿™ä¸ªåˆ†ææ˜¯å¦å‡†ç¡®"
}}"""

        return f"{base_system}\n\n{role_prompt}"

    def _build_user_prompt(self, prompt: OrchestratorPrompt) -> str:
        """æ„å»ºç”¨æˆ·æç¤º - å‡å°‘ä¸Šä¸‹æ–‡çˆ†ç‚¸"""
        # æ ¼å¼åŒ–å¯¹è¯å†å²ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
        transcript_text = self._format_transcript_window(prompt.transcript)

        # æ„å»ºç´§å‡‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_text = self._format_context_compact(prompt.context)

        user_prompt = f"""ä»»åŠ¡: {prompt.task}

ä½ çš„è§’è‰²: {prompt.role}
å½“å‰é˜¶æ®µ: {prompt.current_phase.value}

{context_text}

æœ€è¿‘å¯¹è¯:
{transcript_text}

è¯·ä»…è¿”å›ç¬¦åˆschemaçš„JSONå›å¤ï¼Œæ— å…¶ä»–æ–‡æœ¬:"""

        return user_prompt

    def _format_transcript_window(self, transcript) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å² - çª—å£åŒ–æ˜¾ç¤º"""
        if not transcript:
            return "(å¯¹è¯å¼€å§‹)"

        # åªæ˜¾ç¤ºæœ€è¿‘3è½®ï¼Œé¿å…ä¸Šä¸‹æ–‡çˆ†ç‚¸
        recent_turns = transcript[-3:]
        formatted = []

        for i, turn in enumerate(recent_turns):
            role_icon = "ğŸ”§" if turn.role == "executor" else "ğŸ‘" if turn.role == "reviewer" else "ğŸ¯"
            message = turn.reply['message'][:150] + "..." if len(turn.reply['message']) > 150 else turn.reply['message']
            formatted.append(f"{role_icon} {turn.role}: {message}")

            # æ˜¾ç¤ºå·¥å…·ç»“æœæ‘˜è¦
            if turn.tool_results:
                tool_summary = f"[å·¥å…·: {len(turn.tool_results)}ä¸ªç»“æœ]"
                formatted.append(f"   â””â”€â”€ {tool_summary}")

        return "\n".join(formatted)

    def _format_context_compact(self, context: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯ - ç´§å‡‘æ˜¾ç¤º"""
        parts = []

        # å…³é”®ä¿¡æ¯ä¼˜å…ˆ
        if context.get("running_summary"):
            summary = context["running_summary"][:200] + "..." if len(context["running_summary"]) > 200 else context["running_summary"]
            parts.append(f"å…³é”®æ‘˜è¦: {summary}")

        if context.get("last_critique"):
            critique = context["last_critique"][:100] + "..." if len(context["last_critique"]) > 100 else context["last_critique"]
            parts.append(f"æœ€è¿‘å®¡æ ¸: {critique}")

        # ç»Ÿè®¡ä¿¡æ¯
        stats = f"è¿›åº¦: {context.get('turn_count', 0)}/{context.get('max_turns', 10)} è½®"
        if context.get('total_tokens'):
            stats += f" | Tokens: {context['total_tokens']}"
        parts.append(stats)

        return "\n".join(parts) if parts else ""

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä»£ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "total_tokens": self.token_count,
            "retry_count": self.retry_count
        }


class RetryableAgent(BaseAgent):
    """
    æ”¯æŒé”™è¯¯æ¢å¤çš„ä»£ç†åŸºç±»
    å®ç°Codexå»ºè®®çš„çº é”™ä¸æ¢å¤æœºåˆ¶
    """

    async def generate_with_recovery(self, prompt: OrchestratorPrompt) -> AgentReply:
        """
        å¸¦çº é”™å›åˆçš„ç”Ÿæˆæ–¹æ³•
        """
        # é¦–æ¬¡å°è¯•
        result = await self.generate(prompt)

        # å¦‚æœæ˜¯é”™è¯¯å›å¤ï¼Œå°è¯•è‡ªä¿®å¤
        if "[ç³»ç»Ÿé”™è¯¯]" in result["message"]:
            recovery_prompt = self._create_recovery_prompt(prompt, result["message"])
            result = await self.generate(recovery_prompt)

        return result

    def _create_recovery_prompt(self, original_prompt: OrchestratorPrompt, error_msg: str) -> OrchestratorPrompt:
        """
        åˆ›å»ºæ¢å¤æç¤º - å¼•å¯¼æ¨¡å‹è‡ªä¿®å¤
        """
        recovery_context = original_prompt.context.copy()
        recovery_context["error_feedback"] = f"ä¸Šæ¬¡å›å¤æ ¼å¼é”™è¯¯: {error_msg}ã€‚è¯·ä¸¥æ ¼æŒ‰JSON schemaè¿”å›ã€‚"

        return OrchestratorPrompt(
            task=original_prompt.task,
            role=original_prompt.role,
            current_phase=original_prompt.current_phase,
            transcript=original_prompt.transcript,
            context=recovery_context
        )